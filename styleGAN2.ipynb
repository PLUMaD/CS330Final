{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1a2969-1bc7-4026-b11b-9dec21955f4a",
   "metadata": {},
   "source": [
    "Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4580b7e-be2e-4b71-810e-92aef04e5343",
   "metadata": {},
   "source": [
    "I was the only group member for this project, Daniel Ma. \n",
    "\n",
    "I had found resources for training my own StyleGAN2 model and wanted to work on attempting that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874ebdd-2734-4e9d-aeb3-df4b837b1b9b",
   "metadata": {},
   "source": [
    "StyleGAN2 is a Generative Adversarial Network.\n",
    "Google developers had great resources to give an overview of GANs. \n",
    "Here is the link for what I had read from\n",
    "https://developers.google.com/machine-learning/gan/gan_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a08522-70d6-4441-8cec-9802c34e06cb",
   "metadata": {},
   "source": [
    "I also learnt about this from Dr. Caley in the Robotic Agents course. I had been very confused when thrown several concepts of algorithms and names that I had never heard of before as a sophomore in that class. Seeing a GAN charted out made me recall learning about it in that class and feeling completely lost. \n",
    "\n",
    "Having worked on it here today, it is very validating on my personal growth as a software developer that I feel like I understand both what general concepts are applied in making a GAN work and also that, with time, I can make an operable model for something that was once entirely foreign to myself.\n",
    "\n",
    "The me in Sophomore year would have had no clue how to troubleshoot anything, but during this project, when encountering bugs in the process of trying to train the model, I had actively been able to consider where problems in the process may be and how to use debugging tools effectively to confirm them.\n",
    "\n",
    "Unfortunately, I hadn't been able to get the code to work by the end, but I feel significantly more confident after having the opportunity to challenge myself with something that was once too much for me to remotely understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b0601-9f15-4beb-87ee-c6ab9bcf2e4f",
   "metadata": {},
   "source": [
    "During the project I initially used the code from https://nn.labml.ai/gan/stylegan/index.html\n",
    "\n",
    "The above is explicitly detailing how to make a small model that should not require very many resources.\n",
    "This was more manageable for myself as I wanted to try to train the model on my computer.\n",
    "StyleGAN2 would require more resources as it learns to make progressively larger images.\n",
    "\n",
    "The dataset used in the above was not available for me, so I used a cat dataset instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe5c94-8be4-4714-8068-fa121d3f0651",
   "metadata": {},
   "source": [
    "Additionally, I found that there was a lot of background to be learnt about styleGAN2 from the research paper by the 3 creators from NVIDIA.\n",
    "I have a link to that paper below.\n",
    "https://arxiv.org/abs/1812.04948"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b882c7d-c17b-4efc-9391-cdcaf31d04c5",
   "metadata": {},
   "source": [
    "Studying GANs, I note that several concepts that we learnt in class were very clear to see in the idea of a GAN and had corresponding segments in styleGAN2.\n",
    "\n",
    "The concept of epochs and steps in the training process are immediately brought up in the description of GANs that was in the google developer articles. In addition, studying GANs made it so that I was able to understand loss and gradient descent in the training process better as I had to go over how GANs can have two loss functions that relate to each other for the Generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4fe7c41-e9b3-4761-b1d8-1ba1277aa9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: labml in c:\\users\\dmath\\new folder\\lib\\site-packages (0.4.168)\n",
      "Requirement already satisfied: gitpython in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml) (3.1.40)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml) (6.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml) (1.24.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dmath\\new folder\\lib\\site-packages (from gitpython->labml) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\dmath\\new folder\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: labml_helpers in c:\\users\\dmath\\new folder\\lib\\site-packages (0.4.89)\n",
      "Requirement already satisfied: labml>=0.4.158 in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_helpers) (0.4.168)\n",
      "Requirement already satisfied: torch in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_helpers) (2.1.1)\n",
      "Requirement already satisfied: gitpython in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml>=0.4.158->labml_helpers) (3.1.40)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml>=0.4.158->labml_helpers) (6.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml>=0.4.158->labml_helpers) (1.24.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_helpers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_helpers) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_helpers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_helpers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_helpers) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_helpers) (2023.9.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dmath\\new folder\\lib\\site-packages (from gitpython->labml>=0.4.158->labml_helpers) (4.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dmath\\new folder\\lib\\site-packages (from jinja2->torch->labml_helpers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dmath\\new folder\\lib\\site-packages (from sympy->torch->labml_helpers) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\dmath\\new folder\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.158->labml_helpers) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: labml_nn in c:\\users\\dmath\\new folder\\lib\\site-packages (0.4.136)\n",
      "Requirement already satisfied: labml==0.4.168 in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_nn) (0.4.168)\n",
      "Requirement already satisfied: labml-helpers==0.4.89 in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_nn) (0.4.89)\n",
      "Requirement already satisfied: torch in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_nn) (2.1.1)\n",
      "Requirement already satisfied: torchtext in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_nn) (0.16.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_nn) (0.16.1)\n",
      "Requirement already satisfied: einops in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_nn) (0.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_nn) (1.24.3)\n",
      "Requirement already satisfied: fairscale in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml_nn) (0.4.13)\n",
      "Requirement already satisfied: gitpython in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml==0.4.168->labml_nn) (3.1.40)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dmath\\new folder\\lib\\site-packages (from labml==0.4.168->labml_nn) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_nn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_nn) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_nn) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_nn) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_nn) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dmath\\new folder\\lib\\site-packages (from torch->labml_nn) (2023.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dmath\\new folder\\lib\\site-packages (from torchtext->labml_nn) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dmath\\new folder\\lib\\site-packages (from torchtext->labml_nn) (2.31.0)\n",
      "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\dmath\\new folder\\lib\\site-packages (from torchtext->labml_nn) (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\dmath\\new folder\\lib\\site-packages (from torchdata==0.7.1->torchtext->labml_nn) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dmath\\new folder\\lib\\site-packages (from torchvision->labml_nn) (9.5.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dmath\\new folder\\lib\\site-packages (from gitpython->labml==0.4.168->labml_nn) (4.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dmath\\new folder\\lib\\site-packages (from jinja2->torch->labml_nn) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dmath\\new folder\\lib\\site-packages (from requests->torchtext->labml_nn) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dmath\\new folder\\lib\\site-packages (from requests->torchtext->labml_nn) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dmath\\new folder\\lib\\site-packages (from requests->torchtext->labml_nn) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dmath\\new folder\\lib\\site-packages (from sympy->torch->labml_nn) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dmath\\new folder\\lib\\site-packages (from tqdm->torchtext->labml_nn) (0.4.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\dmath\\new folder\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython->labml==0.4.168->labml_nn) (5.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install labml\n",
    "!pip3 install labml_helpers\n",
    "!pip3 install labml_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca08e26b-7fbf-4aed-8e35-cefbb316df05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "from labml import tracker, lab, monit, experiment\n",
    "from labml.configs import BaseConfigs\n",
    "from labml_helpers.device import DeviceConfigs\n",
    "from labml_helpers.train_valid import ModeState, hook_model_outputs\n",
    "from labml_nn.gan.stylegan import Discriminator, Generator, MappingNetwork, GradientPenalty, PathLengthPenalty\n",
    "from labml_nn.gan.wasserstein import DiscriminatorLoss, GeneratorLoss\n",
    "from labml_nn.utils import cycle_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fdc2845-4f19-463f-94c3-d0badb39aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ## Dataset\n",
    "\n",
    "    This loads the training dataset and resize it to the give image size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, image_size: int):\n",
    "        \"\"\"\n",
    "        * `path` path to the folder containing the images\n",
    "        * `image_size` size of the image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Get the paths of all `jpg` files\n",
    "        self.paths = [p for p in Path(path).glob(f'**/*.jpg')]\n",
    "\n",
    "        # Transformation\n",
    "        self.transform = torchvision.transforms.Compose([\n",
    "            # Resize the image\n",
    "            torchvision.transforms.Resize(image_size),\n",
    "            # Convert to PyTorch tensor\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of images\"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get the `index`-th image\"\"\"\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path)\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a256845-1f4b-4a1c-a1fa-6403df332063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec0fa4c880c4c49981bcc2f498e87e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\\n<span style=\"color: #C5C1B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Configs(BaseConfigs):\n",
    "    \"\"\"\n",
    "    ## Configurations\n",
    "    \"\"\"\n",
    "\n",
    "    # Device to train the model on.\n",
    "    # [`DeviceConfigs`](https://docs.labml.ai/api/helpers.html#labml_helpers.device.DeviceConfigs)\n",
    "    #  picks up an available CUDA device or defaults to CPU.\n",
    "    device: torch.device = DeviceConfigs()\n",
    "\n",
    "    # [StyleGAN2 Discriminator](index.html#discriminator)\n",
    "    discriminator: Discriminator\n",
    "    # [StyleGAN2 Generator](index.html#generator)\n",
    "    generator: Generator\n",
    "    # [Mapping network](index.html#mapping_network)\n",
    "    mapping_network: MappingNetwork\n",
    "\n",
    "    # Discriminator and generator loss functions.\n",
    "    # We use [Wasserstein loss](../wasserstein/index.html)\n",
    "    discriminator_loss: DiscriminatorLoss\n",
    "    generator_loss: GeneratorLoss\n",
    "\n",
    "    # Optimizers\n",
    "    generator_optimizer: torch.optim.Adam\n",
    "    discriminator_optimizer: torch.optim.Adam\n",
    "    mapping_network_optimizer: torch.optim.Adam\n",
    "\n",
    "    # [Gradient Penalty Regularization Loss](index.html#gradient_penalty)\n",
    "    gradient_penalty = GradientPenalty()\n",
    "    # Gradient penalty coefficient $\\gamma$\n",
    "    gradient_penalty_coefficient: float = 10.\n",
    "\n",
    "    # [Path length penalty](index.html#path_length_penalty)\n",
    "    path_length_penalty: PathLengthPenalty\n",
    "\n",
    "    # Data loader\n",
    "    loader: Iterator\n",
    "\n",
    "    # Batch size\n",
    "    batch_size: int = 32\n",
    "    # Dimensionality of $z$ and $w$\n",
    "    d_latent: int = 512\n",
    "    # Height/width of the image\n",
    "    image_size: int = 32\n",
    "    # Number of layers in the mapping network\n",
    "    mapping_network_layers: int = 8\n",
    "    # Generator & Discriminator learning rate\n",
    "    learning_rate: float = 1e-3\n",
    "    # Mapping network learning rate ($100 \\times$ lower than the others)\n",
    "    mapping_network_learning_rate: float = 1e-5\n",
    "    # Number of steps to accumulate gradients on. Use this to increase the effective batch size.\n",
    "    gradient_accumulate_steps: int = 1\n",
    "    # $\\beta_1$ and $\\beta_2$ for Adam optimizer\n",
    "    adam_betas: Tuple[float, float] = (0.0, 0.99)\n",
    "    # Probability of mixing styles\n",
    "    style_mixing_prob: float = 0.9\n",
    "\n",
    "    # Total number of training steps\n",
    "    training_steps: int = 150_000\n",
    "\n",
    "    # Number of blocks in the generator (calculated based on image resolution)\n",
    "    n_gen_blocks: int\n",
    "\n",
    "    # ### Lazy regularization\n",
    "    # Instead of calculating the regularization losses, the paper proposes lazy regularization\n",
    "    # where the regularization terms are calculated once in a while.\n",
    "    # This improves the training efficiency a lot.\n",
    "\n",
    "    # The interval at which to compute gradient penalty\n",
    "    lazy_gradient_penalty_interval: int = 4\n",
    "    # Path length penalty calculation interval\n",
    "    lazy_path_penalty_interval: int = 32\n",
    "    # Skip calculating path length penalty during the initial phase of training\n",
    "    lazy_path_penalty_after: int = 5_000\n",
    "\n",
    "    # How often to log generated images\n",
    "    log_generated_interval: int = 500\n",
    "    # How often to save model checkpoints\n",
    "    save_checkpoint_interval: int = 2_000\n",
    "\n",
    "    # Training mode state for logging activations\n",
    "    mode: ModeState\n",
    "    # Whether to log model layer outputs\n",
    "    log_layer_outputs: bool = False\n",
    "\n",
    "    # <a id=\"dataset_path\"></a>\n",
    "    # We trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\n",
    "    # You can find the download instruction in this\n",
    "    # [discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n",
    "    # Save the images inside `data/stylegan` folder.\n",
    "    dataset_path: str = str(lab.get_data_path() / 'stylegan2')\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        ### Initialize\n",
    "        \"\"\"\n",
    "        # Create dataset\n",
    "        dataset = Dataset(self.dataset_path, self.image_size)\n",
    "        # Create data loader\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, num_workers=8,\n",
    "                                                 shuffle=True, drop_last=True, pin_memory=True)\n",
    "        # Continuous [cyclic loader](../../utils.html#cycle_dataloader)\n",
    "        self.loader = cycle_dataloader(dataloader)\n",
    "\n",
    "        # $\\log_2$ of image resolution\n",
    "        log_resolution = int(math.log2(self.image_size))\n",
    "\n",
    "        # Create discriminator and generator\n",
    "        self.discriminator = Discriminator(log_resolution).to(self.device)\n",
    "        self.generator = Generator(log_resolution, self.d_latent).to(self.device)\n",
    "        # Get number of generator blocks for creating style and noise inputs\n",
    "        self.n_gen_blocks = self.generator.n_blocks\n",
    "        # Create mapping network\n",
    "        self.mapping_network = MappingNetwork(self.d_latent, self.mapping_network_layers).to(self.device)\n",
    "        # Create path length penalty loss\n",
    "        self.path_length_penalty = PathLengthPenalty(0.99).to(self.device)\n",
    "\n",
    "        # Add model hooks to monitor layer outputs\n",
    "        if self.log_layer_outputs:\n",
    "            hook_model_outputs(self.mode, self.discriminator, 'discriminator')\n",
    "            hook_model_outputs(self.mode, self.generator, 'generator')\n",
    "            hook_model_outputs(self.mode, self.mapping_network, 'mapping_network')\n",
    "\n",
    "        # Discriminator and generator losses\n",
    "        self.discriminator_loss = DiscriminatorLoss().to(self.device)\n",
    "        self.generator_loss = GeneratorLoss().to(self.device)\n",
    "\n",
    "        # Create optimizers\n",
    "        self.discriminator_optimizer = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=self.learning_rate, betas=self.adam_betas\n",
    "        )\n",
    "        self.generator_optimizer = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=self.learning_rate, betas=self.adam_betas\n",
    "        )\n",
    "        self.mapping_network_optimizer = torch.optim.Adam(\n",
    "            self.mapping_network.parameters(),\n",
    "            lr=self.mapping_network_learning_rate, betas=self.adam_betas\n",
    "        )\n",
    "\n",
    "        # Set tracker configurations\n",
    "        tracker.set_image(\"generated\", True)\n",
    "\n",
    "    def get_w(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        ### Sample $w$\n",
    "\n",
    "        This samples $z$ randomly and get $w$ from the mapping network.\n",
    "\n",
    "        We also apply style mixing sometimes where we generate two latent variables\n",
    "        $z_1$ and $z_2$ and get corresponding $w_1$ and $w_2$.\n",
    "        Then we randomly sample a cross-over point and apply $w_1$ to\n",
    "        the generator blocks before the cross-over point and\n",
    "        $w_2$ to the blocks after.\n",
    "        \"\"\"\n",
    "\n",
    "        # Mix styles\n",
    "        if torch.rand(()).item() < self.style_mixing_prob:\n",
    "            # Random cross-over point\n",
    "            cross_over_point = int(torch.rand(()).item() * self.n_gen_blocks)\n",
    "            # Sample $z_1$ and $z_2$\n",
    "            z2 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
    "            z1 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
    "            # Get $w_1$ and $w_2$\n",
    "            w1 = self.mapping_network(z1)\n",
    "            w2 = self.mapping_network(z2)\n",
    "            # Expand $w_1$ and $w_2$ for the generator blocks and concatenate\n",
    "            w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n",
    "            w2 = w2[None, :, :].expand(self.n_gen_blocks - cross_over_point, -1, -1)\n",
    "            return torch.cat((w1, w2), dim=0)\n",
    "        # Without mixing\n",
    "        else:\n",
    "            # Sample $z$ and $z$\n",
    "            z = torch.randn(batch_size, self.d_latent).to(self.device)\n",
    "            # Get $w$ and $w$\n",
    "            w = self.mapping_network(z)\n",
    "            # Expand $w$ for the generator blocks\n",
    "            return w[None, :, :].expand(self.n_gen_blocks, -1, -1)\n",
    "\n",
    "    def get_noise(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        ### Generate noise\n",
    "\n",
    "        This generates noise for each [generator block](index.html#generator_block)\n",
    "        \"\"\"\n",
    "        # List to store noise\n",
    "        noise = []\n",
    "        # Noise resolution starts from $4$\n",
    "        resolution = 4\n",
    "\n",
    "        # Generate noise for each generator block\n",
    "        for i in range(self.n_gen_blocks):\n",
    "            # The first block has only one $3 \\times 3$ convolution\n",
    "            if i == 0:\n",
    "                n1 = None\n",
    "            # Generate noise to add after the first convolution layer\n",
    "            else:\n",
    "                n1 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
    "            # Generate noise to add after the second convolution layer\n",
    "            n2 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
    "\n",
    "            # Add noise tensors to the list\n",
    "            noise.append((n1, n2))\n",
    "\n",
    "            # Next block has $2 \\times$ resolution\n",
    "            resolution *= 2\n",
    "\n",
    "        # Return noise tensors\n",
    "        return noise\n",
    "\n",
    "    def generate_images(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        ### Generate images\n",
    "\n",
    "        This generate images using the generator\n",
    "        \"\"\"\n",
    "\n",
    "        # Get $w$\n",
    "        w = self.get_w(batch_size)\n",
    "        # Get noise\n",
    "        noise = self.get_noise(batch_size)\n",
    "\n",
    "        # Generate images\n",
    "        images = self.generator(w, noise)\n",
    "\n",
    "        # Return images and $w$\n",
    "        return images, w\n",
    "\n",
    "    def step(self, idx: int):\n",
    "        \"\"\"\n",
    "        ### Training Step\n",
    "        \"\"\"\n",
    "\n",
    "        # Train the discriminator\n",
    "        with monit.section('Discriminator'):\n",
    "            # Reset gradients\n",
    "            self.discriminator_optimizer.zero_grad()\n",
    "\n",
    "            # Accumulate gradients for `gradient_accumulate_steps`\n",
    "            for i in range(self.gradient_accumulate_steps):\n",
    "                # Update `mode`. Set whether to log activation\n",
    "                with self.mode.update(is_log_activations=(idx + 1) % self.log_generated_interval == 0):\n",
    "                    # Sample images from generator\n",
    "                    generated_images, _ = self.generate_images(self.batch_size)\n",
    "                    # Discriminator classification for generated images\n",
    "                    fake_output = self.discriminator(generated_images.detach())\n",
    "\n",
    "                    # Get real images from the data loader\n",
    "                    real_images = next(self.loader).to(self.device)\n",
    "                    # We need to calculate gradients w.r.t. real images for gradient penalty\n",
    "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
    "                        real_images.requires_grad_()\n",
    "                    # Discriminator classification for real images\n",
    "                    real_output = self.discriminator(real_images)\n",
    "\n",
    "                    # Get discriminator loss\n",
    "                    real_loss, fake_loss = self.discriminator_loss(real_output, fake_output)\n",
    "                    disc_loss = real_loss + fake_loss\n",
    "\n",
    "                    # Add gradient penalty\n",
    "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
    "                        # Calculate and log gradient penalty\n",
    "                        gp = self.gradient_penalty(real_images, real_output)\n",
    "                        tracker.add('loss.gp', gp)\n",
    "                        # Multiply by coefficient and add gradient penalty\n",
    "                        disc_loss = disc_loss + 0.5 * self.gradient_penalty_coefficient * gp * self.lazy_gradient_penalty_interval\n",
    "\n",
    "                    # Compute gradients\n",
    "                    disc_loss.backward()\n",
    "\n",
    "                    # Log discriminator loss\n",
    "                    tracker.add('loss.discriminator', disc_loss)\n",
    "\n",
    "            if (idx + 1) % self.log_generated_interval == 0:\n",
    "                # Log discriminator model parameters occasionally\n",
    "                tracker.add('discriminator', self.discriminator)\n",
    "\n",
    "            # Clip gradients for stabilization\n",
    "            torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n",
    "            # Take optimizer step\n",
    "            self.discriminator_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        with monit.section('Generator'):\n",
    "            # Reset gradients\n",
    "            self.generator_optimizer.zero_grad()\n",
    "            self.mapping_network_optimizer.zero_grad()\n",
    "\n",
    "            # Accumulate gradients for `gradient_accumulate_steps`\n",
    "            for i in range(self.gradient_accumulate_steps):\n",
    "                # Sample images from generator\n",
    "                generated_images, w = self.generate_images(self.batch_size)\n",
    "                # Discriminator classification for generated images\n",
    "                fake_output = self.discriminator(generated_images)\n",
    "\n",
    "                # Get generator loss\n",
    "                gen_loss = self.generator_loss(fake_output)\n",
    "\n",
    "                # Add path length penalty\n",
    "                if idx > self.lazy_path_penalty_after and (idx + 1) % self.lazy_path_penalty_interval == 0:\n",
    "                    # Calculate path length penalty\n",
    "                    plp = self.path_length_penalty(w, generated_images)\n",
    "                    # Ignore if `nan`\n",
    "                    if not torch.isnan(plp):\n",
    "                        tracker.add('loss.plp', plp)\n",
    "                        gen_loss = gen_loss + plp\n",
    "\n",
    "                # Calculate gradients\n",
    "                gen_loss.backward()\n",
    "\n",
    "                # Log generator loss\n",
    "                tracker.add('loss.generator', gen_loss)\n",
    "\n",
    "            if (idx + 1) % self.log_generated_interval == 0:\n",
    "                # Log discriminator model parameters occasionally\n",
    "                tracker.add('generator', self.generator)\n",
    "                tracker.add('mapping_network', self.mapping_network)\n",
    "\n",
    "            # Clip gradients for stabilization\n",
    "            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(self.mapping_network.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Take optimizer step\n",
    "            self.generator_optimizer.step()\n",
    "            self.mapping_network_optimizer.step()\n",
    "\n",
    "        # Log generated images\n",
    "        if (idx + 1) % self.log_generated_interval == 0:\n",
    "            tracker.add('generated', torch.cat([generated_images[:6], real_images[:3]], dim=0))\n",
    "        # Save model checkpoints\n",
    "        if (idx + 1) % self.save_checkpoint_interval == 0:\n",
    "            experiment.save_checkpoint()\n",
    "\n",
    "        # Flush tracker\n",
    "        tracker.save()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        ## Train model\n",
    "        \"\"\"\n",
    "\n",
    "        # Loop for `training_steps`\n",
    "        for i in monit.loop(self.training_steps):\n",
    "            # Take a training step\n",
    "            self.step(i)\n",
    "            #\n",
    "            if (i + 1) % self.log_generated_interval == 0:\n",
    "                tracker.new_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a60b51-e197-4bbf-bc26-49d7e4573ab5",
   "metadata": {},
   "source": [
    "I would perform debugging in pycharm, but the error message appears below her in the notebook\n",
    "The error with missized tensors was the most recent that I saw from the project, but I had to deal with either w(latent space vectors) or the noise having a list passed to generate_images with a list index out of bounds error. Then, in the github code, I was getting varying tensor sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef78578-7168-4aea-b550-026ab9e00795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2988aa8609bb497ea778257b8d5880de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\\n<span style=\"color: #C5C1B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dmath\\New folder\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 13848, 8108, 16140, 15132, 11964, 18380, 5344, 1152) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\New folder\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Start the experiment\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mstart():\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Run the training loop\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 346\u001b[0m, in \u001b[0;36mConfigs.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# Loop for `training_steps`\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m monit\u001b[38;5;241m.\u001b[39mloop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_steps):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;66;03m# Take a training step\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_generated_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[4], line 250\u001b[0m, in \u001b[0;36mConfigs.step\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    247\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(generated_images\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Get real images from the data loader\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m real_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# We need to calculate gradients w.r.t. real images for gradient penalty\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_gradient_penalty_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\labml_nn\\utils\\__init__.py:35\u001b[0m, in \u001b[0;36mcycle_dataloader\u001b[1;34m(data_loader)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m<a id=\"cycle_dataloader\"></a>\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mInfinite loader that recycles the data loader after each epoch\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\New folder\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 13848, 8108, 16140, 15132, 11964, 18380, 5344, 1152) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    ### Train StyleGAN2\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an experiment\n",
    "    experiment.create(name='stylegan2')\n",
    "    # Create configurations object\n",
    "    configs = Configs()\n",
    "\n",
    "    # Set configurations and override some\n",
    "    experiment.configs(configs, {\n",
    "        'device.cuda_device': 0,\n",
    "        'image_size': 64,\n",
    "        'log_generated_interval': 200\n",
    "    })\n",
    "\n",
    "    # Initialize\n",
    "    configs.init()\n",
    "    # Set models for saving and loading\n",
    "    experiment.add_pytorch_models(mapping_network=configs.mapping_network,\n",
    "                                  generator=configs.generator,\n",
    "                                  discriminator=configs.discriminator)\n",
    "\n",
    "    # Start the experiment\n",
    "    with experiment.start():\n",
    "        # Run the training loop\n",
    "        configs.train()\n",
    "\n",
    "\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc84f48-48f1-4238-8fe1-6f681456c97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
